{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b674cb00",
   "metadata": {},
   "source": [
    "#### Entrega GoMotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b701b8",
   "metadata": {},
   "source": [
    "Realizamos los imports necesarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "592b2889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. Obtener la ruta absoluta del directorio actual del notebook\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# 2. Subir un nivel (o los necesarios) para llegar a la raíz del proyecto\n",
    "# Si tu notebook está en 'GoMotion/notebooks', el padre es 'GoMotion'\n",
    "project_root = os.path.dirname(current_dir)\n",
    "\n",
    "# 3. Añadir la raíz al sys.path\n",
    "# Esto permite hacer imports como \"from src import ...\" aunque estés en una subcarpeta\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# 4. Cambiar el directorio de trabajo a la raíz\n",
    "# Esto arregla los rutas relativas a archivos tipo \"./data/archivo.csv\"\n",
    "os.chdir(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c5118a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'intensities'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m os.environ[\u001b[33m'\u001b[39m\u001b[33mTF_CPP_MIN_LOG_LEVEL\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33m3\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mintensities\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mevent_encoder\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllm_scraper\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'intensities'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#shut tensorflow up if it doesn't need to say anything\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import pandas as pd\n",
    "import src.intensities as intensities\n",
    "import src.event_encoder as event_encoder\n",
    "import src.llm_scraper as llm_scraper\n",
    "import datetime\n",
    "import src.data_filler as data_filler\n",
    "import src.hyperparameter_optimizer as hyperparameter_optimizer\n",
    "import keras\n",
    "import numpy as np\n",
    "from src.metadata_manager import MetadataManager\n",
    "from src.pipeline import check_and_load_data, process_scraped_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f7d27f",
   "metadata": {},
   "source": [
    "Empezamos a ejecutar el pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7349fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = MetadataManager()\n",
    "TODAY = datetime.datetime.today()\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4d64ea",
   "metadata": {},
   "source": [
    "Computamos las intensidades de los días que ya tenemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d0a6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"../data/intensities.csv\"):\n",
    "    # we only load the merged data if we do not have the intensities, as once we have them there is no need to load this (big file)\n",
    "    merged_data = check_and_load_data(False)\n",
    "    # calculate the intensities for each day-barri pair according to the mathematical formulation of the task\n",
    "    intensities_df = intensities.process_df(merged_data, download=True, verbose=1)\n",
    "else:\n",
    "    intensities_df = pd.read_csv(\"../data/intensities.csv\")\n",
    "    # keep this (its useful)\n",
    "    barri_list = intensities_df[\"barri\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e277241e",
   "metadata": {},
   "source": [
    "Cargamos los datos de eventos y vacaciones. Pasamos los eventos por el codificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69163688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with encoder\n",
    "encoder_created = False\n",
    "if not os.path.exists(\"data/encoded_events.csv\"):\n",
    "    if not os.path.exists(\"data/events.csv\"):\n",
    "        raise Exception(\"Event data is missing\")\n",
    "    if not os.path.exists(\"data/holidays.csv\"):\n",
    "        raise Exception(\"Event data is missing\")\n",
    "\n",
    "    # load precomputed events and holidays, rename them and keep track of it in metadata\n",
    "    all_events = pd.read_csv(\"data/events.csv\")\n",
    "    all_events.to_csv(\"data/all_events.csv\", index=None)\n",
    "    all_holidays = pd.read_csv(\"data/holidays.csv\")\n",
    "    all_holidays.to_csv(\"data/all_holidays.csv\", index=None)\n",
    "\n",
    "    # run event_encoder.py\n",
    "    # the file creates an encoder (which takes existing events and projects them to a 5-dimensional latent space conserving all information) and processes all events for given data\n",
    "    event_encoder.main(manager)\n",
    "\n",
    "    # keep track of this; if the encoder is rebuilt, the weights might have changed, so we need to retrain XGB model\n",
    "    encoder_created = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272fa402",
   "metadata": {},
   "source": [
    "Si ejecutamos este pipeline más de una vez, es posible que falten eventos o vacaciones futuras (el scrapper solo recoge eventos con una semana de antelación), que tendremos que recopilar con nuestro scrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce4f2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part only matters if the pipeline is run multiple times: since the day might be different, we need to check new events\n",
    "if (\n",
    "    datetime.datetime.strptime(\n",
    "        manager.get(\"last_day_event_checked\"), \"%Y-%m-%d\"\n",
    "    ).date()\n",
    "    < TODAY.date()\n",
    "):\n",
    "    # there might be new events\n",
    "    (new_events, new_holidays) = process_scraped_events(\n",
    "        llm_scraper.scrape_week_ahead(),\n",
    "        barri_list,\n",
    "    )\n",
    "\n",
    "    # load old events and holidays, concatenate new ones and metadata\n",
    "    all_events = pd.read_csv(\"data/all_events.csv\")\n",
    "    all_holidays = pd.read_csv(\"data/all_holidays.csv\")\n",
    "\n",
    "    # cut events and holidays that might already be loaded\n",
    "    event_cutoff = max(all_events[\"day\"])\n",
    "    holiday_cutoff = max(all_holidays[\"day\"])\n",
    "\n",
    "    new_events = new_events[new_events[\"day\"] > event_cutoff]\n",
    "    new_holidays = new_holidays[new_holidays[\"day\"] > holiday_cutoff]\n",
    "\n",
    "    all_events = pd.concat([all_events, new_events])\n",
    "    all_holidays = pd.concat([all_holidays, new_holidays])\n",
    "    all_events.to_csv(\"data/all_events.csv\", index=None)\n",
    "    all_holidays.to_csv(\"data/all_holidays.csv\", index=None)\n",
    "    manager.set(\"last_day_event_checked\", TODAY.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    # need to encode new events\n",
    "    encoder = keras.models.load_model(\"models/encoder.keras\")\n",
    "    encoder_max_len = int(manager.get(\"encoder_max_len\"))\n",
    "\n",
    "    # predict no events (need bias)\n",
    "    if len(new_events) > 0:\n",
    "        encoded_events = pd.read_csv(\"data/encoded_events.csv\")\n",
    "        new_encoded_events = event_encoder.predict(\n",
    "            new_events, encoder, encoder_max_len, 5\n",
    "        )\n",
    "        encoded_events = pd.concat([encoded_events, new_encoded_events])\n",
    "        encoded_events.to_csv(\"data/encoded_events.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db900cfc",
   "metadata": {},
   "source": [
    "Creamos el modelo predictivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13abf613",
   "metadata": {},
   "outputs": [],
   "source": [
    "if encoder_created or not os.path.exists(\"models/regressor.joblib\"):\n",
    "        # we need to train a model. For that, we first prepare the data\n",
    "        data = intensities_df\n",
    "        data[\"day\"] = pd.to_datetime(data[\"day\"])\n",
    "\n",
    "        # add meteo first\n",
    "        data = data_filler.add_weather_features(\n",
    "            data, datetime.datetime(year=2022, month=12, day=31), datetime.date.today()\n",
    "        )\n",
    "\n",
    "        # add rest of features\n",
    "        data_processed = hyperparameter_optimizer.create_features(data)\n",
    "\n",
    "        # store features for all\n",
    "        df_to_save = hyperparameter_optimizer.create_features(data, False)\n",
    "        df_to_save.drop(inplace=True, columns=[\"enc1\", \"enc2\", \"enc3\", \"enc4\", \"enc5\"])\n",
    "        df_to_save.to_csv(\n",
    "            \"data/data_processed.csv\", index=None\n",
    "        )\n",
    "\n",
    "        # training portion\n",
    "        # choose where to split the dataset\n",
    "        split_date = datetime.datetime(year=2025, month=1, day=1)\n",
    "\n",
    "        # split data\n",
    "        train = data_processed.loc[data_processed[\"day\"] < split_date].copy()\n",
    "        test = data_processed.loc[data_processed[\"day\"] >= split_date].copy()\n",
    "\n",
    "        # remove nans\n",
    "        train = train.dropna()\n",
    "\n",
    "        # define empty hyperspace; adding values next\n",
    "        hyperspace = []\n",
    "\n",
    "         # weights base\n",
    "        hyperspace.append([10])\n",
    "        # learning rate\n",
    "        hyperspace.append([0.0005])\n",
    "        # tree depth\n",
    "        hyperspace.append([9])\n",
    "        hyperparameter_optimizer.grid_search(\n",
    "            hyperspace, 0, [], hyperparameter_optimizer.features, train, test\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f4f65b",
   "metadata": {},
   "source": [
    "Abrimos el dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2d4972",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"streamlit run src/dashboard.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
